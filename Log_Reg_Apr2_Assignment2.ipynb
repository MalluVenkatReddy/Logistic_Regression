{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d969f9f-fcae-4d21-9f3a-d5b0da781dfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Q1. What is the purpose of grid search cv in machine learning, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fdc8ac-84be-459d-afc8-0ba9d0166c6a",
   "metadata": {},
   "source": [
    "#### GridSearchCV is a technique for finding the optimal parameter values from a given set of parameters in a grid. It's essentially a cross-validation technique. The model as well as the parameters must be entered. After extracting the best parameter values, predictions are made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865b0695-c9d2-4ff1-9ebf-20ed93b441f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose \n",
    "one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7b05db-fcc5-41ce-8d6e-677829136764",
   "metadata": {},
   "source": [
    "#### In Grid Search, we try every combination of a preset list of values of the hyper-parameters and choose the best combination based on the cross-validation score. Random search tries random combinations of a range of values (we have to define the number iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b371bc94-a5c1-44c4-99ac-4852fd7c6aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is data leakage, and why is it a problem in machine learning? Provide an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9374784-6595-4fb4-900b-4c34e8a4fb08",
   "metadata": {},
   "source": [
    "#### Data leakage (or leakage) happens when your training data contains information about the target, but similar data will not be available when the model is used for prediction. This leads to high performance on the training set (and possibly even the validation data), but the model will perform poorly in production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5088de1-8d93-4a43-ae7e-b690964bea68",
   "metadata": {},
   "source": [
    "#### Data leakage can be multi-million dollar mistake in many data science applications. Careful separation of training and validation data can prevent train-test contamination, and pipelines can help implement this separation. Likewise, a combination of caution, common sense, and data exploration can help identify target leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4940a3-9d3c-4bb7-b5e1-1c6885745936",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. How can you prevent data leakage when building a machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681327fe-edf7-45b8-b4a3-91227e2d471c",
   "metadata": {},
   "source": [
    "#### One of the best ways to get rid of data leakage is to perform k-fold cross validation where the overall data is divided into k parts. After dividing into k parts, we use each part as the cross-validation data and the remaining as training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645bbb86-d84c-4b03-a52b-035476afdb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d522100f-07c0-45a1-be34-64a568b6c943",
   "metadata": {},
   "source": [
    "#### A confusion matrix is a table that allows you to visualize the performance of a classification model. You can also use the information in it to calculate measures that can help you determine the usefulness of the model. Rows represent predicted classifications, while columns represent the true classes from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ae0fc8-b125-4619-8b32-3cfdb9b92996",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Explain the difference between precision and recall in the context of a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c28a7d1-37a7-4f56-8825-7e5e6014eb0e",
   "metadata": {},
   "source": [
    "#### Precision is calculated by dividing the true positives by anything that was predicted as a positive. Recall (or True Positive Rate) is calculated by dividing the true positives by anything that should have been predicted as positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e44dc72-dd4a-48ae-9b9c-f1625567da68",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c77b95-06c9-4ad8-a079-520ca1cf3a44",
   "metadata": {},
   "source": [
    "#### False Positive (FP) – Type I Error\n",
    "\n",
    "##### The predicted value was falsely predicted.\n",
    "\n",
    "--The actual value was negative, but the model predicted a positive value. Also known as the type I error.\n",
    "\n",
    "#### False Negative (FN) – Type II Error\n",
    "\n",
    "##### The predicted value was falsely predicted.\n",
    "\n",
    "-- The actual value was positive, but the model predicted a negative value. Also known as the type II error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea72242-9b89-46e6-b6ff-88db6a5e427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. What are some common metrics that can be derived from a confusion matrix, and how are they \n",
    "calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b2c6c4-f2b6-44cf-8fa4-3fd176a0e144",
   "metadata": {},
   "source": [
    "#### Confusion matrices can be used to calculate performance metrics for classification models. Of the many performance metrics used, the most common are accuracy, precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d938cc6-ff81-4afe-a18c-a62a15be403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855ba370-1b07-402a-9022-fb7d68b4eed1",
   "metadata": {},
   "source": [
    "#### Accuracy: It gives you the overall accuracy of the model, meaning the fraction of the total samples that were correctly classified by the classifier. To calculate accuracy, use the following formula: (TP+TN)/(TP+TN+FP+FN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5482a9-1eb6-46e7-8a43-89b35678eb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning \n",
    "model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6914109a-ed2f-4e48-8660-7f9b13e5a1f8",
   "metadata": {},
   "source": [
    "#### A confusion matrix prints the correct and also incorrect values in number count . It helps us for a good Data Visualization It gives us insight not only into the errors being made by a classifier but more importantly the types of errors that are being made."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
